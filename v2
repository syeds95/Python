I’ve added a small L2 penalizer to the CoxPH fit (to resolve collinearity), and wrapped the fit in a try/except so it won’t crash your pipeline. Here’s the complete, end-to-end script with that fix included:

```python
#!/usr/bin/env python3
"""
Patient-Level Forecasting & Evaluation Pipeline (Serial, Full Version)

Features:
  - Baseline models: Naive & Mean
  - Time-series models: ETS, SARIMA
  - Survival model: CoxPH on inter-order gaps with L2 penalization
  - Skip series with ≤2 orders in training
  - 12→12 backtesting
  - 12-event production forecasts
  - Loads MODEL_EVAL & EVAL_DETAIL into Snowflake

Assumes `my_df` loaded with:
  PATIENTID, ORDERNUMBER, HCPCS, SERVICEDATEOFORDER,
  TOTALAMOUNT, TOTALQTY, SUPPLYDURATION, ELIGIBLETOSHIP

Dependencies:
  pandas, numpy, statsmodels, pmdarima, lifelines, scikit-learn, snowflake-connector-python
"""
import os
import logging
from datetime import timedelta
import pandas as pd
import numpy as np
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from pmdarima import auto_arima
from lifelines import CoxPHFitter
from lifelines.exceptions import ConvergenceError
from sklearn.metrics import mean_absolute_error, mean_squared_error
from snowflake.connector import connect
from snowflake.connector.pandas_tools import write_pandas

# --- Logging ---
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# --- 0. Load your data into my_df ---
# Replace with your loader:
# conn = connect(...)
# my_df = pd.read_sql(..., conn)
logger.info("Assuming `my_df` is loaded with required columns.")

# --- 1. Preprocess ---
my_df = my_df.rename(columns={
    'SERVICEDATEOFORDER': 'date',
    'TOTALQTY':           'units'
})
my_df['date']           = pd.to_datetime(my_df['date'])
my_df['units']          = my_df['units'].astype(float)
my_df['ELIGIBLETOSHIP'] = pd.to_datetime(my_df['ELIGIBLETOSHIP'])
my_df['HCPCS']          = my_df['HCPCS'].astype(str)
logger.info("Preprocessing complete.")

# --- 2. Fit global CoxPH on inter-order gaps ---
gap_records = []
for (pid, code), grp in my_df.groupby(['PATIENTID', 'HCPCS']):
    dates = sorted(grp['date'])
    for prev, nxt in zip(dates, dates[1:]):
        gap_records.append({
            'eligible_days': grp.loc[grp.date == prev, 'SUPPLYDURATION'].iloc[0],
            'duration':      (nxt - prev).days,
            'event':         1,
            'HCPCS':         code
        })

cph = None
if gap_records:
    gap_df = pd.DataFrame(gap_records)
    gap_df = pd.get_dummies(gap_df, columns=['HCPCS'], drop_first=True)

    try:
        # Add a small penalizer to avoid singularities from collinearity
        cph = CoxPHFitter(penalizer=0.1)
        cph.fit(gap_df, duration_col='duration', event_col='event', show_progress=False)
        logger.info("CoxPH model fitted with L2 penalization.")
    except ConvergenceError as e:
        logger.warning("CoxPH failed to converge (collinearity?): %s; skipping CoxPH.", e)
        cph = None
else:
    logger.warning("No gap records found; skipping CoxPH fit.")

# --- 3. Forecasting helpers ---
def fit_naive(train, horizon):
    val = train.iloc[-1] if len(train) > 0 else np.nan
    return pd.Series(val, index=horizon)

def fit_mean(train, horizon):
    val = train.mean() if len(train) > 0 else np.nan
    return pd.Series(val, index=horizon)

def fit_ets(train, horizon):
    try:
        m = ExponentialSmoothing(train, trend='add', seasonal=None).fit()
        return pd.Series(m.forecast(len(horizon)), index=horizon)
    except:
        return pd.Series(np.nan, index=horizon)

def fit_sarima(train, horizon):
    try:
        p = int(train.diff().dropna().median()) or 1
        m = auto_arima(train, seasonal=True, m=p, suppress_warnings=True)
        return pd.Series(m.predict(len(horizon)), index=horizon)
    except:
        return pd.Series(np.nan, index=horizon)

# --- 4. Model selection function ---
def select_best(pid, code, train, test, cph_model):
    horizon = test.index
    preds = {
        'Naive':  fit_naive(train, horizon),
        'Mean':   fit_mean(train, horizon),
        'ETS':    fit_ets(train, horizon),
        'SARIMA': fit_sarima(train, horizon),
    }
    errors = {}
    if len(test) > 0:
        for name, p in preds.items():
            if len(p) == len(test):
                errors[name] = mean_absolute_error(test, p)
    # Add CoxPH if available and enough history
    if cph_model and (train > 0).sum() > 2:
        last_date = train[train > 0].index.max()
        last_sup  = my_df.loc[
            (my_df.PATIENTID == pid) &
            (my_df.HCPCS == code) &
            (my_df.date == last_date),
            'SUPPLYDURATION'
        ].iloc[0]
        cov_cols = list(cph_model.params_.index)
        cov_dict = {col: 0 for col in cov_cols}
        cov_dict['eligible_days'] = last_sup
        hcpc_col = f'HCPCS_{code}'
        if hcpc_col in cov_dict:
            cov_dict[hcpc_col] = 1
        cov = pd.DataFrame([cov_dict])
        _pm = cph_model.predict_median(cov)
        med = float(_pm.values[0]) if hasattr(_pm, 'values') else float(_pm)
        errors['CoxPH'] = med
    if not errors:
        return 'Mean', preds['Mean']
    best = min(errors, key=errors.get)
    return best, (preds.get(best) if best in preds else None)

# --- 5. Backtest (12→12), skipping small series ---
logger.info("Starting 12→12 backtest (skipping ≤2 orders)...")
keys = my_df[['PATIENTID', 'HCPCS']].drop_duplicates().values.tolist()
eval_records = []

for pid, code in keys:
    grp = my_df[(my_df.PATIENTID == pid) & (my_df.HCPCS == code)]
    ts  = grp.set_index('date')['units'].resample('D').sum().fillna(0)
    if ts.empty:
        continue

    start      = ts.index.min()
    train_end  = start + pd.DateOffset(months=12) - pd.Timedelta(days=1)
    test_start = train_end + pd.Timedelta(days=1)
    test_end   = test_start + pd.DateOffset(months=12) - pd.Timedelta(days=1)

    train = ts[start:train_end]
    test  = ts[test_start:test_end]

    # skip series with too few orders
    if (train > 0).sum() <= 2:
        best   = 'Mean'
        pred   = fit_mean(train, test.index)
        mae_val  = (np.nan if test.empty else mean_absolute_error(test, pred))
        rmse_val = (np.nan if test.empty else mean_squared_error(test, pred)**0.5)
    else:
        best, pred = select_best(pid, code, train, test, cph)
        mae_val  = (np.nan if (pred is None or test.empty) else mean_absolute_error(test, pred))
        rmse_val = (np.nan if (pred is None or test.empty) else mean_squared_error(test, pred)**0.5)

    eval_records.append({
        'PATIENTID':      pid,
        'HCPCS':          code,
        'MODEL_SELECTED': best,
        'MAE_12MO':       mae_val,
        'RMSE_12MO':      rmse_val
    })

eval_df = pd.DataFrame(eval_records)
logger.info("Backtest complete: %d series.", len(eval_df))

# --- 6. Production forecast (next 12 events) ---
logger.info("Starting production forecast...")
fc_records = []

for pid, code in keys:
    grp = my_df[(my_df.PATIENTID == pid) & (my_df.HCPCS == code)]
    ts  = grp.set_index('date')['units'].resample('D').sum().fillna(0)
    if ts.empty:
        continue

    last_ship = grp['ELIGIBLETOSHIP'].max() if grp['ELIGIBLETOSHIP'].notna().any() else ts.index.max()
    train     = ts[:last_ship]
    horizon   = pd.date_range(start=last_ship + timedelta(days=1), periods=365, freq='D')

    best = eval_df.loc[
        (eval_df.PATIENTID == pid) & (eval_df.HCPCS == code),
        'MODEL_SELECTED'
    ].iloc[0]

    if best == 'CoxPH' and cph:
        last_sup = grp.loc[grp.date == last_ship, 'SUPPLYDURATION'].iloc[0]
        cov_cols = list(cph.params_.index)
        cov_dict = {col: 0 for col in cov_cols}
        cov_dict['eligible_days'] = last_sup
        hcpc_col = f'HCPCS_{code}'
        if hcpc_col in cov_dict:
            cov_dict[hcpc_col] = 1
        cov = pd.DataFrame([cov_dict])
        _pm = cph.predict_median(cov)
        med = float(_pm.values[0]) if hasattr(_pm, 'values') else float(_pm)
        dates = [last_ship + timedelta(days=int(med * i)) for i in range(1, 13)]
        for i, d in enumerate(dates, 1):
            fc_records.append({
                'PATIENTID':    pid,
                'HCPCS':        code,
                'SERVICE_NUM':  i,
                'PRED_DATE':    d.date(),
                'FORECAST_QTY': grp['units'].mean(),
                'MODEL_USED':   'CoxPH'
            })
    else:
        func = {
            'Naive':  fit_naive,
            'Mean':   fit_mean,
            'ETS':    fit_ets,
            'SARIMA': fit_sarima
        }[best]
        series = func(train, horizon)
        ev = series[series > 0].head(12)
        for i, (dt, qty) in enumerate(ev.items(), 1):
            fc_records.append({
                'PATIENTID':    pid,
                'HCPCS':        code,
                'SERVICE_NUM':  i,
                'PRED_DATE':    dt.date(),
                'FORECAST_QTY': float(qty),
                'MODEL_USED':   best
            })

fc_df = pd.DataFrame(fc_records)
logger.info("Forecast complete: %d events.", len(fc_df))

# --- 7. Build evaluation detail ---
act = my_df.sort_values(['PATIENTID','HCPCS','date']).copy()
act['SERVICE_NUM'] = act.groupby(['PATIENTID','HCPCS']).cumcount() + 1
act12 = act[act.SERVICE_NUM <= 12][['PATIENTID','HCPCS','SERVICE_NUM','date','ELIGIBLETOSHIP']]
act12 = act12.rename(columns={'date': 'ACTUAL_DATE', 'ELIGIBLETOSHIP': 'ELIGIBLE_DATE'})

eval_detail = (
    fc_df.merge(act12, on=['PATIENTID','HCPCS','SERVICE_NUM'], how='left')
         .assign(
             ERROR_DAYS=lambda df: (pd.to_datetime(df.PRED_DATE) - pd.to_datetime(df.ACTUAL_DATE)).dt.days,
             DELAY_DAYS=lambda df: (pd.to_datetime(df.ACTUAL_DATE) - pd.to_datetime(df.ELIGIBLE_DATE)).dt.days,
             SLACK_DAYS=lambda df: (pd.to_datetime(df.PRED_DATE) - pd.to_datetime(df.ELIGIBLE_DATE)).dt.days
         )
)
logger.info("Detail evaluation built: %d rows.", len(eval_detail))

# --- 8. Load into Snowflake ---
conn = connect(
    user=os.getenv('SNOWFLAKE_USER'),
    account=os.getenv('SNOWFLAKE_ACCOUNT'),
    authenticator='externalbrowser',
    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
    database=os.getenv('SNOWFLAKE_DATABASE'),
    schema=os.getenv('SNOWFLAKE_SCHEMA')
)
write_pandas(conn, eval_df, 'MODEL_EVAL', schema=os.getenv('SNOWFLAKE_SCHEMA'))
eval_detail['PRED_DATE']     = pd.to_datetime(eval_detail['PRED_DATE']).dt.date
eval_detail['ACTUAL_DATE']   = pd.to_datetime(eval_detail['ACTUAL_DATE']).dt.date
eval_detail['ELIGIBLE_DATE'] = pd.to_datetime(eval_detail['ELIGIBLE_DATE']).dt.date
write_pandas(conn, eval_detail, 'EVAL_DETAIL', schema=os.getenv('SNOWFLAKE_SCHEMA'))
conn.close()
logger.info("Pipeline complete.")
```
