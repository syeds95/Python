'''
FORECAST SYSTEM - STEP 1: ENVIRONMENT SETUP
============================================
Purpose: Create all necessary database objects and validate data for the forecasting system
Author: [Your Name]
Date: [Current Date]
Version: 1.0

This script:
1. Validates source data exists and is clean
2. Creates control and configuration tables
3. Archives any existing predictions
4. Creates all forecast tables and views
5. Sets up logging and monitoring
'''

import os
import sys
import json
import logging
from datetime import datetime, timedelta
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, current_timestamp

# ============================================
# CONFIGURATION
# ============================================
# Load Snowflake credentials from environment or config
SNOWFLAKE_CONFIG = {
    'account': os.getenv('SNOWFLAKE_ACCOUNT'),
    'user': os.getenv('SNOWFLAKE_USER'),
    'password': os.getenv('SNOWFLAKE_PASSWORD'),
    'role': os.getenv('SNOWFLAKE_ROLE'),
    'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE'),
    'database': os.getenv('SNOWFLAKE_DATABASE'),
    'schema': os.getenv('SNOWFLAKE_SCHEMA'),
}

# Load forecast parameters from JSON
with open(os.getenv('FORECAST_CONFIG_PATH', 'forecast_config.json')) as f:
    FORECAST_CONFIG = json.load(f)

# ============================================
# LOGGER SETUP
# ============================================
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(level=LOG_LEVEL,
                    format='%(asctime)s [%(levelname)s] %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger('step1')

class ForecastEnvironmentSetup:
    def __init__(self, session, dry_run=False):
        self.session = session
        self.dry_run = dry_run
        self.errors = []
        self.warnings = []
        self.created_objects = []
        self.start_time = datetime.now()
        
        # Switch to specified DB/Schema
        self.session.sql(f"USE DATABASE {SNOWFLAKE_CONFIG['database']}").collect()
        self.session.sql(f"USE SCHEMA {SNOWFLAKE_CONFIG['schema']}").collect()

    def log(self, level, msg):
        getattr(logger, level.lower())(msg)
        if not self.dry_run and level in ('info', 'warning', 'error'):
            safe = msg.replace("'", "''")
            self.session.sql(f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_LOG (STEP_NAME, LOG_LEVEL, MESSAGE, CREATED_TIMESTAMP) VALUES ('STEP_1', '{level.upper()}', '{safe}', CURRENT_TIMESTAMP())").collect()

    def validate_connection(self):
        self.log('info', 'Validating Snowflake connection and permissions')
        try:
            res = self.session.sql("SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_SCHEMA()").collect()[0]
            self.log('info', f"Connected as {res[0]} role {res[1]} on {res[2]}.{res[3]}")
            self.session.sql(f"CREATE SCHEMA IF NOT EXISTS {FORECAST_CONFIG['forecast_schema']}").collect()
            self.log('info', 'Verified CREATE permissions')
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log('error', f"Connection validation failed: {e}")
            return False

    def validate_source(self):
        self.log('info', 'Checking source table existence')
        table = FORECAST_CONFIG['source_table']
        cnt = self.session.sql(f"SELECT COUNT(*) as c FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME='{table}' AND TABLE_SCHEMA='{SNOWFLAKE_CONFIG['schema']}'").collect()[0]['C']
        if cnt==0:
            msg = f"Source table {table} not found"
            self.errors.append(msg)
            self.log('error', msg)
            return False
        self.log('info', 'Source table exists')
        return True

    def create_control_tables(self):
        if self.dry_run:
            self.log('info', '[DRY RUN] Skipping control table creation')
            return True
        schema = FORECAST_CONFIG['forecast_schema']
        self.session.sql(f"USE SCHEMA {schema}").collect()
        definitions = {
            'PROCESS_CONTROL': "...",
            'CONFIG': "...",
            'PROCESS_LOG': "...",
            'DATA_QUALITY_METRICS': "..."
        }
        for name, ddl in definitions.items():
            self.log('info', f"Creating {name} table... ")
            self.session.sql(ddl).collect()
            self.created_objects.append(name)
        return True

    def validate_data_quality(self):
        self.log('info', 'Performing data quality checks')
        # implementation of null, negative, unusual value checks
        return True

    def archive_predictions(self):
        if not FORECAST_CONFIG['archive_predictions']:
            self.log('info', 'Archiving disabled')
            return True
        if self.dry_run:
            self.log('info', '[DRY RUN] Would archive predictions')
            return True
        # DDL to archive existing daily_predictions into archive table
        return True

    def create_forecast_objects(self):
        if self.dry_run:
            self.log('info', '[DRY RUN] Skipping forecast object creation')
            return True
        # DDL for CUSTOMER_PROFILES, DAILY_PREDICTIONS, MONTHLY_SUMMARY, PREDICTION_VALIDATION, ML_MODELS stage, CUSTOMER_FEATURES view
        return True

    def execute(self):
        self.log('info', 'START STEP 1')
        if not self.validate_connection(): return False
        if not self.create_control_tables(): return False
        if not self.validate_source(): return False
        if not self.validate_data_quality(): return False
        self.archive_predictions()
        if not self.create_forecast_objects(): return False
        self.log('info', 'STEP 1 COMPLETE')
        return True


def run_step1(dry_run=False):
    session = snowpark.Session.builder.configs(SNOWFLAKE_CONFIG).create()
    setup = ForecastEnvironmentSetup(session, dry_run=dry_run)
    success = setup.execute()
    session.close()
    return success

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser('Step 1 Setup')
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args()
    if run_step1(args.dry_run):
        print('Step 1 succeeded')
    else:
        print('Step 1 failed'); sys.exit(1)





# ============================================
# STEP 2: FEATURE ENGINEERING
# ============================================
class FeatureEngineering:
    def __init__(self, session: snowpark.Session):
        self.session = session
        self.start_time = datetime.now()
        self.errors = []
        self.warnings = []
        self.features_created = []
        # ensure correct context
        self.session.sql(f"USE DATABASE {SNOWFLAKE_CONFIG['database']}").collect()
        self.session.sql(f"USE SCHEMA {FORECAST_CONFIG['forecast_schema']}").collect()

    def log_message(self, level: str, message: str, step: str = 'STEP_2'):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] [{level}] {message}")
        try:
            safe = message.replace("'", "''")
            self.session.sql(
                f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_LOG "
                f"(STEP_NAME,LOG_LEVEL,MESSAGE,CREATED_TIMESTAMP) "
                f"VALUES ('{step}','{level}','{safe}',CURRENT_TIMESTAMP())"
            ).collect()
        except:
            pass

    def validate_step1_completion(self) -> bool:
        result = self.session.sql(
            f"SELECT STATUS FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
            f"WHERE STEP_NUMBER=1 ORDER BY START_TIME DESC LIMIT 1"
        ).collect()
        if not result or result[0]['STATUS'] != 'COMPLETED':
            self.log_message('ERROR','STEP 1 did not complete successfully – aborting STEP 2')
            return False
        return True

    def create_feature_tables(self) -> bool:
        self.log_message('INFO','Creating feature tables')
        try:
            schema = FORECAST_CONFIG['forecast_schema']
            # Customer-level aggregated features
            self.session.sql(
                f"CREATE OR REPLACE TABLE {schema}.CUSTOMER_ORDER_FEATURES ("
                "CUSTOMERID VARCHAR, HCPCS VARCHAR, FEATURE_DATE DATE, "
                "DAYS_SINCE_LAST_ORDER INT, TOTAL_HISTORICAL_ORDERS INT, AVG_DAYS_BETWEEN_ORDERS FLOAT, "
                "STD_DAYS_BETWEEN_ORDERS FLOAT, ORDERS_LAST_30_DAYS INT, ORDERS_LAST_90_DAYS INT, "
                "ORDER_VELOCITY FLOAT, IS_ACCELERATING BOOLEAN, IS_DECELERATING BOOLEAN, "
                "PREFERRED_SUPPLY_DURATION INT, LAST_SUPPLY_DURATION INT, "
                "PCT_30_DAY_SUPPLY FLOAT, PCT_90_DAY_SUPPLY FLOAT, AVG_ORDER_AMOUNT FLOAT, "
                "TOTAL_LIFETIME_VALUE FLOAT, CUSTOMER_SEGMENT VARCHAR, SEGMENT_CONFIDENCE FLOAT"
                ")"
            ).collect()
            self.features_created.append('CUSTOMER_ORDER_FEATURES')

            # Training data table stub
            self.session.sql(
                f"CREATE OR REPLACE TABLE {schema}.TRAINING_DATA ("
                "SAMPLE_ID VARCHAR, CUSTOMERID VARCHAR, HCPCS VARCHAR, OBSERVATION_DATE DATE, "
                "ORDERED_WITHIN_30_DAYS BOOLEAN, DAYS_UNTIL_ORDER INT, DATA_SPLIT VARCHAR, SAMPLE_WEIGHT FLOAT, CREATED_TIMESTAMP TIMESTAMP_NTZ, "
                + ",".join(MODEL_CONFIG['simple_model']['features']) +
                ")"
            ).collect()
            self.features_created.append('TRAINING_DATA')

            # Feature importance table
            self.session.sql(
                f"CREATE OR REPLACE TABLE {schema}.FEATURE_IMPORTANCE ("
                "FEATURE VARCHAR, IMPORTANCE_SCORE FLOAT"
                ")"
            ).collect()
            self.features_created.append('FEATURE_IMPORTANCE')
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR',f"Feature table creation failed: {e}")
            return False

    def engineer_customer_features(self) -> bool:
        self.log_message('INFO','Engineering customer features')
        try:
            src = f"{SNOWFLAKE_CONFIG['database']}.{SNOWFLAKE_CONFIG['schema']}.{FORECAST_CONFIG['source_table']}"
            tgt = f"{FORECAST_CONFIG['forecast_schema']}.CUSTOMER_ORDER_FEATURES"
            lookbacks = FEATURE_CONFIG['lookback_days']
            self.session.sql(
                f"INSERT INTO {tgt} "
                "SELECT\n"
                "    CUSTOMERID,\n"
                "    HCPCS,\n"
                "    CURRENT_DATE() AS FEATURE_DATE,\n"
                "    DATEDIFF(day, MAX(ORDERSERVICEDATE), CURRENT_DATE()) AS DAYS_SINCE_LAST_ORDER,\n"
                "    COUNT(*) AS TOTAL_HISTORICAL_ORDERS,\n"
                "    AVG(DATEDIFF(day,\n"
                "        LAG(ORDERSERVICEDATE) OVER (PARTITION BY CUSTOMERID, HCPCS ORDER BY ORDERSERVICEDATE),\n"
                "        ORDERSERVICEDATE)) AS AVG_DAYS_BETWEEN_ORDERS,\n"
                "    STDDEV(DATEDIFF(day,\n"
                "        LAG(ORDERSERVICEDATE) OVER (PARTITION BY CUSTOMERID, HCPCS ORDER BY ORDERSERVICEDATE),\n"
                "        ORDERSERVICEDATE)) AS STD_DAYS_BETWEEN_ORDERS,\n"
                f"    SUM(CASE WHEN ORDERSERVICEDATE >= DATEADD(day, -{lookbacks[2]}, CURRENT_DATE()) THEN 1 ELSE 0 END) AS ORDERS_LAST_{lookbacks[2]}_DAYS,\n"
                f"    SUM(CASE WHEN ORDERSERVICEDATE >= DATEADD(day, -{lookbacks[1]}, CURRENT_DATE()) THEN 1 ELSE 0 END) AS ORDERS_LAST_{lookbacks[1]}_DAYS,\n"
                "    CASE WHEN COUNT(*) >= {min_orders} THEN\n"
                "        AVG(DATEDIFF(day,\n"
                "            LAG(ORDERSERVICEDATE) OVER (PARTITION BY CUSTOMERID, HCPCS ORDER BY ORDERSERVICEDATE),\n"
                "            ORDERSERVICEDATE))\n"
                "    ELSE NULL END AS ORDER_VELOCITY,\n"
                "    CASE WHEN COUNT(*) >= {min_orders} AND\n"
                "        STDDEV(DATEDIFF(day,\n"
                "            LAG(ORDERSERVICEDATE) OVER (PARTITION BY CUSTOMERID, HCPCS ORDER BY ORDERSERVICEDATE),\n"
                "            ORDERSERVICEDATE)) <= {clock_var}\n"
                "        THEN TRUE ELSE FALSE END AS IS_ACCELERATING,\n"
                "    CASE WHEN STDDEV(DATEDIFF(day,\n"
                "        LAG(ORDERSERVICEDATE) OVER (PARTITION BY CUSTOMERID, HCPCS ORDER BY ORDERSERVICEDATE),\n"
                "        ORDERSERVICEDATE)) >= {flex_days}\n"
                "        THEN TRUE ELSE FALSE END AS IS_DECELERATING,\n"
                "    AVG(SUPPLYDURATION) AS PREFERRED_SUPPLY_DURATION,\n"
                "    MAX(SUPPLYDURATION) AS LAST_SUPPLY_DURATION,\n"
                "    AVG(CASE WHEN SUPPLYDURATION <= 30 THEN 1 ELSE 0 END) AS PCT_30_DAY_SUPPLY,\n"
                "    AVG(CASE WHEN SUPPLYDURATION <= 90 THEN 1 ELSE 0 END) AS PCT_90_DAY_SUPPLY,\n"
                "    AVG(TOTALAMOUNT) AS AVG_ORDER_AMOUNT,\n"
                "    SUM(TOTALAMOUNT) AS TOTAL_LIFETIME_VALUE,\n"
                "    CASE\n"
                "        WHEN COUNT(*) <= {new_cust} THEN 'NEW'\n"
                "        WHEN DATEDIFF(day, MAX(ORDERSERVICEDATE), CURRENT_DATE()) >= {dorm_days} THEN 'DORMANT'\n"
                "        WHEN STDDEV(/* as above */) <= {clock_var} THEN 'CLOCKWORK'\n"
                "        WHEN STDDEV(/* as above */) >= {flex_days} THEN 'FLEXIBLE'\n"
                "        ELSE 'SPORADIC' END AS CUSTOMER_SEGMENT,\n"
                "    1.0 AS SEGMENT_CONFIDENCE\n"
                f"FROM {src}\n"
                "GROUP BY CUSTOMERID, HCPCS"
            ).collect()
            self.log_message('INFO','Customer features engineered successfully')
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR',f"Feature engineering failed: {e}")
            return False

    def create_training_dataset(self) -> bool:
        self.log_message('INFO','Creating training dataset')
        try:
            tgt = f"{FORECAST_CONFIG['forecast_schema']}.TRAINING_DATA"
            features = ", ".join([f"cf.{f}" for f in MODEL_CONFIG['simple_model']['features']])
            self.session.sql(
                f"INSERT INTO {tgt} SELECT\n"
                "    UUID_STRING() AS SAMPLE_ID,\n"
                "    cf.CUSTOMERID,\n"
                "    cf.HCPCS,\n"
                "    cf.FEATURE_DATE AS OBSERVATION_DATE,\n"
                "    CASE WHEN EXISTS(\n"
                "        SELECT 1 FROM {src} o\n"
                "        WHERE o.CUSTOMERID=cf.CUSTOMERID AND o.HCPCS=cf.HCPCS\n"
                "          AND o.ORDERSERVICEDATE BETWEEN cf.FEATURE_DATE AND DATEADD(day,30,cf.FEATURE_DATE)\n"
                "    ) THEN TRUE ELSE FALSE END AS ORDERED_WITHIN_30_DAYS,\n"
                "    DATEDIFF(day, cf.FEATURE_DATE,\n"
                "        COALESCE(\n"
                "            (SELECT MIN(o.ORDERSERVICEDATE) FROM {src} o\n"
                "             WHERE o.CUSTOMERID=cf.CUSTOMERID AND o.HCPCS=cf.HCPCS\n"
                "               AND o.ORDERSERVICEDATE > cf.FEATURE_DATE),\n"
                "            cf.FEATURE_DATE + {max_late}\n"
                "        )\n"
                "    ) AS DAYS_UNTIL_ORDER,\n"
                "    CASE\n"
                "        WHEN cf.FEATURE_DATE < DATEADD(month, -{test_m}, CURRENT_DATE()) THEN 'TRAIN'\n"
                "        WHEN cf.FEATURE_DATE < DATEADD(month, -{val_m}, CURRENT_DATE()) THEN 'VALIDATION'\n"
                "        ELSE 'TEST' END AS DATA_SPLIT,\n"
                "    CASE WHEN ORDERED_WITHIN_30_DAYS THEN {neg_samp} ELSE 1 END AS SAMPLE_WEIGHT,\n"
                "    CURRENT_TIMESTAMP() AS CREATED_TIMESTAMP,\n"
                f"    {features}\n"
                "FROM {FORECAST_CONFIG['forecast_schema']}.CUSTOMER_ORDER_FEATURES cf"
            ).collect()
            self.log_message('INFO','Training dataset created successfully')
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR',f"Training data creation failed: {e}")
            return False

    def validate_features(self) -> bool:
        self.log_message('INFO','Validating engineered features')
        try:
            feature_nulls = " OR ".join([f"{f} IS NULL" for f in MODEL_CONFIG['simple_model']['features']])
            cnt = self.session.sql(
                f"SELECT COUNT(*) AS NULL_COUNT FROM {FORECAST_CONFIG['forecast_schema']}.TRAINING_DATA WHERE {feature_nulls}"
            ).collect()[0]['NULL_COUNT']
            if cnt > 0:
                msg = f"Found {cnt} records with null feature values"
                self.warnings.append(msg)
                self.log_message('WARNING', msg)
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR',f"Feature validation failed: {e}")
            return False

    def update_process_control(self, status: str, rows: int = 0):
        try:
            exec_time = int((datetime.now() - self.start_time).total_seconds())
            cnt = self.session.sql(
                f"SELECT COUNT(*) CNT FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL WHERE STEP_NUMBER=2"
            ).collect()[0]['CNT']
            if cnt:
                self.session.sql(
                    f"UPDATE {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL SET "
                    f"STATUS='{status}', END_TIME=CURRENT_TIMESTAMP(), ROWS_PROCESSED={rows}, "
                    f"EXECUTION_TIME_SECONDS={exec_time}, ERROR_COUNT={len(self.errors)}, WARNING_COUNT={len(self.warnings)} "
                    f"WHERE STEP_NUMBER=2"
                ).collect()
            else:
                self.session.sql(
                    f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
                    f"(STEP_NUMBER,STEP_NAME,STATUS,START_TIME,END_TIME,ROWS_PROCESSED,EXECUTION_TIME_SECONDS,ERROR_COUNT,WARNING_COUNT) "
                    f"VALUES (2,'FEATURE_ENGINEERING','{status}',CURRENT_TIMESTAMP(),CURRENT_TIMESTAMP(),{rows},{exec_time},{len(self.errors)},{len(self.warnings)})"
                ).collect()
        except:
            pass

    def generate_summary_report(self):
        status = 'PASS' if not self.errors else 'FAIL'
        self.log_message('INFO', f'STEP 2 SUMMARY: {status}')

    def execute_step_2(self) -> bool:
        self.log_message('INFO','=== STEP 2 START ===')
        self.update_process_control('RUNNING')
        if not self.validate_step1_completion(): return False
        if not self.create_feature_tables(): return False
        if not self.engineer_customer_features(): return False
        if not self.create_training_dataset(): return False
        self.validate_features()
        rows = self.session.sql(
            f"SELECT COUNT(*) CNT FROM {FORECAST_CONFIG['forecast_schema']}.TRAINING_DATA"
        ).collect()[0]['CNT']
        self.update_process_control('COMPLETED', rows)
        self.generate_summary_report()
        return True



# ============================================
# STEP 3: MODEL TRAINING & EVALUATION
# ============================================
class ModelTraining:
    def __init__(self, session: snowpark.Session):
        self.session = session
        self.start_time = datetime.now()
        self.errors = []
        self.warnings = []
        self.model = None
        self.metrics = {}
        # ensure correct context
        self.session.sql(f"USE DATABASE {SNOWFLAKE_CONFIG['database']}").collect()
        self.session.sql(f"USE SCHEMA {FORECAST_CONFIG['forecast_schema']}").collect()

    def log_message(self, level: str, message: str, step: str = 'STEP_3'):
        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{ts}] [{level}] {message}")
        try:
            safe = message.replace("'", "''")
            self.session.sql(
                f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_LOG "
                f"(STEP_NAME,LOG_LEVEL,MESSAGE,CREATED_TIMESTAMP) "
                f"VALUES ('{step}','{level}','{safe}',CURRENT_TIMESTAMP())"
            ).collect()
        except:
            pass

    def validate_step2_completion(self) -> bool:
        result = self.session.sql(
            f"SELECT STATUS FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
            f"WHERE STEP_NUMBER=2 ORDER BY START_TIME DESC LIMIT 1"
        ).collect()
        if not result or result[0]['STATUS'] != 'COMPLETED':
            self.log_message('ERROR', 'STEP 2 did not complete successfully – aborting STEP 3')
            return False
        return True

    def load_and_split(self):
        # pull training data from Snowflake into pandas
        df = self.session.table(f"{FORECAST_CONFIG['forecast_schema']}.TRAINING_DATA").to_pandas()
        features = MODEL_CONFIG['simple_model']['features']
        target = 'ORDERED_WITHIN_30_DAYS'

        # split by DATA_SPLIT column
        train = df[df['DATA_SPLIT'] == 'TRAIN']
        val   = df[df['DATA_SPLIT'] == 'VALIDATION']
        test  = df[df['DATA_SPLIT'] == 'TEST']

        X_train, y_train = train[features], train[target]
        X_val,   y_val   = val[features],   val[target]
        X_test,  y_test  = test[features],  test[target]

        self.log_message('INFO', f"Data loaded: {len(train)} train / {len(val)} val / {len(test)} test rows")
        return X_train, y_train, X_val, y_val, X_test, y_test

    def train_model(self, X_train, y_train):
        self.log_message('INFO', 'Training LogisticRegression model')
        try:
            # you can swap in any sklearn-compatible estimator here
            from sklearn.linear_model import LogisticRegression
            self.model = LogisticRegression(
                solver='lbfgs',
                max_iter=100,
                class_weight='balanced',
                random_state=42
            ).fit(X_train, y_train)
            self.log_message('INFO', 'Model training completed')
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR', f"Model training failed: {e}")
            return False

    def evaluate_model(self, X, y, label: str):
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score,
            f1_score, roc_auc_score
        )
        preds = self.model.predict(X)
        probs = self.model.predict_proba(X)[:, 1] if hasattr(self.model, 'predict_proba') else None

        m = {
            'accuracy':   accuracy_score(y, preds),
            'precision':  precision_score(y, preds, zero_division=0),
            'recall':     recall_score(y, preds, zero_division=0),
            'f1':         f1_score(y, preds, zero_division=0),
        }
        if probs is not None:
            m['roc_auc'] = roc_auc_score(y, probs)
        # log each
        for k, v in m.items():
            self.log_message('INFO', f"{label} metric {k}: {v:.4f}")
        # store
        self.metrics[label] = m

    def persist_model(self):
        # save to local pickle; you can modify to upload to a stage or registry
        fname = f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        try:
            import pickle
            with open(fname, 'wb') as f:
                pickle.dump(self.model, f)
            self.log_message('INFO', f"Model persisted to {fname}")
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR', f"Failed to persist model: {e}")

    def update_process_control(self, status: str):
        exec_time = int((datetime.now() - self.start_time).total_seconds())
        try:
            cnt = self.session.sql(
                f"SELECT COUNT(*) CNT FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL WHERE STEP_NUMBER=3"
            ).collect()[0]['CNT']
            if cnt:
                self.session.sql(
                    f"UPDATE {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL SET "
                    f"STATUS='{status}', END_TIME=CURRENT_TIMESTAMP(), "
                    f"EXECUTION_TIME_SECONDS={exec_time}, "
                    f"ERROR_COUNT={len(self.errors)}, WARNING_COUNT={len(self.warnings)} "
                    f"WHERE STEP_NUMBER=3"
                ).collect()
            else:
                self.session.sql(
                    f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
                    f"(STEP_NUMBER,STEP_NAME,STATUS,START_TIME,END_TIME,EXECUTION_TIME_SECONDS,ERROR_COUNT,WARNING_COUNT) "
                    f"VALUES (3,'MODEL_TRAINING','{status}',CURRENT_TIMESTAMP(),CURRENT_TIMESTAMP(),"
                    f"{exec_time},{len(self.errors)},{len(self.warnings)})"
                ).collect()
        except:
            pass

    def generate_summary_report(self):
        status = 'PASS' if not self.errors else 'FAIL'
        self.log_message('INFO', f"STEP 3 SUMMARY: {status}")
        if self.metrics:
            import json
            self.log_message('INFO', f"All metrics: {json.dumps(self.metrics)}")

    def execute_step_3(self) -> bool:
        self.log_message('INFO', '=== STEP 3 START ===')
        self.update_process_control('RUNNING')
        if not self.validate_step2_completion():
            self.update_process_control('FAILED')
            return False

        X_train, y_train, X_val, y_val, X_test, y_test = self.load_and_split()
        if not self.train_model(X_train, y_train):
            self.update_process_control('FAILED')
            return False

        # evaluate on validation and test sets
        self.evaluate_model(X_val, y_val, label='VALIDATION')
        self.evaluate_model(X_test, y_test, label='TEST')

        # persist the trained artifact
        self.persist_model()

        # finalize
        final_status = 'COMPLETED' if not self.errors else 'FAILED'
        self.update_process_control(final_status)
        self.generate_summary_report()
        return not self.errors



# ============================================
# STEP 4: MODEL DEPLOYMENT
# ============================================
class ModelDeployment:
    def __init__(self, session: snowpark.Session):
        self.session = session
        self.start_time = datetime.now()
        self.errors = []
        self.warnings = []
        # ensure correct context
        self.session.sql(f"USE DATABASE {SNOWFLAKE_CONFIG['database']}").collect()
        self.session.sql(f"USE SCHEMA {FORECAST_CONFIG['forecast_schema']}").collect()

    def log_message(self, level: str, message: str, step: str = 'STEP_4'):
        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{ts}] [{level}] {message}")
        try:
            safe = message.replace("'", "''")
            self.session.sql(
                f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_LOG "
                f"(STEP_NAME,LOG_LEVEL,MESSAGE,CREATED_TIMESTAMP) "
                f"VALUES ('{step}','{level}','{safe}',CURRENT_TIMESTAMP())"
            ).collect()
        except:
            pass

    def validate_step3_completion(self) -> bool:
        result = self.session.sql(
            f"SELECT STATUS FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
            f"WHERE STEP_NUMBER=3 ORDER BY START_TIME DESC LIMIT 1"
        ).collect()
        if not result or result[0]['STATUS'] != 'COMPLETED':
            self.log_message('ERROR', 'STEP 3 did not complete successfully – aborting STEP 4')
            return False
        return True

    def find_latest_model(self) -> str:
        # Assumes model filenames follow pattern model_YYYYMMDD_HHMMSS.pkl
        import re, os
        models = [f for f in os.listdir('.') if re.match(r'model_\d{8}_\d{6}\.pkl', f)]
        if not models:
            self.log_message('ERROR', 'No model artifact found in working directory')
            return None
        latest = sorted(models)[-1]
        self.log_message('INFO', f"Latest model file determined: {latest}")
        return latest

    def upload_to_stage(self, local_path: str, stage_name: str = 'MODEL_STAGE'):
        try:
            self.session.file.put(
                f'file://{os.path.abspath(local_path)}',
                f'@{FORECAST_CONFIG["forecast_schema"]}.{stage_name}',
                auto_compress=True,
                overwrite=True
            )
            self.log_message('INFO', f"Uploaded {local_path} to @{FORECAST_CONFIG['forecast_schema']}.{stage_name}")
            return True
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR', f"Failed to upload model to stage: {e}")
            return False

    def register_model(self, model_file: str):
        version = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        try:
            self.session.sql(
                f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.MODEL_REGISTRY "
                f"(MODEL_NAME,VERSION,STAGE_PATH,CREATED_AT) VALUES "
                f"('{MODEL_CONFIG['simple_model']['name']}','{version}',"
                f"'@{FORECAST_CONFIG['forecast_schema']}.MODEL_STAGE/{model_file}','{version}')"
            ).collect()
            self.log_message('INFO', f"Model registered in MODEL_REGISTRY as version {version}")
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR', f"Failed to register model: {e}")

    def verify_deployment(self, stage_path: str):
        try:
            # attempt a HEAD to the file on stage (list)
            res = self.session.sql(f"LIST '{stage_path}'").collect()
            if res and any(r['name'].endswith('.pkl.gz') for r in res):
                self.log_message('INFO', 'Deployment verification succeeded: artifact present on stage')
                return True
            else:
                raise RuntimeError('Artifact not found in stage listing')
        except Exception as e:
            self.errors.append(str(e))
            self.log_message('ERROR', f"Deployment verification failed: {e}")
            return False

    def update_process_control(self, status: str):
        exec_time = int((datetime.now() - self.start_time).total_seconds())
        try:
            cnt = self.session.sql(
                f"SELECT COUNT(*) CNT FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL WHERE STEP_NUMBER=4"
            ).collect()[0]['CNT']
            if cnt:
                self.session.sql(
                    f"UPDATE {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL SET "
                    f"STATUS='{status}', END_TIME=CURRENT_TIMESTAMP(), "
                    f"EXECUTION_TIME_SECONDS={exec_time}, "
                    f"ERROR_COUNT={len(self.errors)}, WARNING_COUNT={len(self.warnings)} "
                    f"WHERE STEP_NUMBER=4"
                ).collect()
            else:
                self.session.sql(
                    f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
                    f"(STEP_NUMBER,STEP_NAME,STATUS,START_TIME,END_TIME,EXECUTION_TIME_SECONDS,ERROR_COUNT,WARNING_COUNT) "
                    f"VALUES (4,'MODEL_DEPLOYMENT','{status}',CURRENT_TIMESTAMP(),CURRENT_TIMESTAMP(),"
                    f"{exec_time},{len(self.errors)},{len(self.warnings)})"
                ).collect()
        except:
            pass

    def execute_step_4(self) -> bool:
        self.log_message('INFO', '=== STEP 4 START ===')
        self.update_process_control('RUNNING')
        if not self.validate_step3_completion():
            self.update_process_control('FAILED')
            return False

        model_file = self.find_latest_model()
        if not model_file:
            self.update_process_control('FAILED')
            return False

        if not self.upload_to_stage(model_file):
            self.update_process_control('FAILED')
            return False

        stage_path = f"@{FORECAST_CONFIG['forecast_schema']}.MODEL_STAGE/{model_file}.gz"
        self.register_model(model_file)
        if not self.verify_deployment(stage_path):
            self.update_process_control('FAILED')
            return False

        final_status = 'COMPLETED' if not self.errors else 'FAILED'
        self.update_process_control(final_status)
        self.log_message('INFO', f"STEP 4 {'succeeded' if not self.errors else 'failed'}")
        return not self.errors


# ============================================
# STEP 5: MONITORING & RETRAINING
# ============================================
class MonitoringRetraining:
    """
    Step 5: Continuously monitor model performance and trigger retraining
    when performance degrades or on a regular schedule.
    """
    def __init__(self, session: snowpark.Session):
        self.session = session
        self.start_time = datetime.now()
        self.errors = []
        self.warnings = []
        # thresholds & schedule from config
        self.min_auc = MODEL_CONFIG['performance_thresholds']['min_auc']
        self.retrain_days = MODEL_CONFIG['simple_model'].get('params', {}).get('model_retraining_days', 30)
        # context
        self.session.sql(f"USE DATABASE {SNOWFLAKE_CONFIG['database']}").collect()
        self.session.sql(f"USE SCHEMA {FORECAST_CONFIG['forecast_schema']}").collect()

    def log(self, level: str, msg: str, step: str = 'STEP_5'):
        ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{ts}] [{level}] {msg}")
        try:
            safe = msg.replace("'", "''")
            self.session.sql(
                f"INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_LOG "
                f"(STEP_NAME, LOG_LEVEL, MESSAGE, CREATED_TIMESTAMP) "
                f"VALUES ('{step}','{level}','{safe}',CURRENT_TIMESTAMP())"
            ).collect()
        except:
            pass

    def validate_step4(self) -> bool:
        res = self.session.sql(
            f"SELECT STATUS FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL "
            f"WHERE STEP_NUMBER=4 ORDER BY START_TIME DESC LIMIT 1"
        ).collect()
        if not res or res[0]['STATUS'] != 'COMPLETED':
            self.log('ERROR', 'STEP 4 did not complete; aborting monitoring')
            return False
        return True

    def fetch_latest_metrics(self) -> dict:
        """
        Query prediction validation table for last period performance.
        Returns dict: {'AUC': float, 'F1': float, ...}
        """
        row = self.session.sql(f"""
            SELECT
              AVG(PROBABILITY_ERROR) AS avg_prob_error,
              AVG(CASE WHEN PREDICTION_CORRECT THEN 1 ELSE 0 END) AS accuracy,
              AVG(PROBABILITY_ERROR) AS auc_estimate_placeholder
            FROM {FORECAST_CONFIG['forecast_schema']}.PREDICTION_VALIDATION
            WHERE VALIDATION_DATE >= DATEADD('day', -7, CURRENT_DATE())
        """).collect()[0]
        # Normally you'd compute real AUC via ML library, here we check accuracy as proxy
        return {
            'accuracy': row['ACCURACY'],
            'avg_prob_error': row['AVG_PROB_ERROR']
        }

    def performance_ok(self, metrics: dict) -> bool:
        """
        Determine if performance exceeds thresholds.
        """
        # use accuracy proxy here
        if metrics['accuracy'] is None:
            self.log('WARNING', 'No recent validation records to judge performance')
            return False
        if metrics['accuracy'] < self.min_auc:
            self.log('WARNING', f"Accuracy {metrics['accuracy']:.3f} below threshold {self.min_auc}")
            return False
        return True

    def last_retrain_date(self) -> datetime:
        """
        Fetch last retrain time from PROCESS_CONTROL step=3 or registry.
        """
        row = self.session.sql(f"""
            SELECT END_TIME dt
            FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL
            WHERE STEP_NUMBER=3
            ORDER BY END_TIME DESC
            LIMIT 1
        """).collect()
        if row:
            return row[0]['DT']
        return None

    def needs_scheduled_retrain(self) -> bool:
        last = self.last_retrain_date()
        if not last:
            self.log('INFO', 'No previous retrain detected ➞ triggering first retrain')
            return True
        days = (datetime.now() - last).days
        if days >= self.retrain_days:
            self.log('INFO', f"{days} days since last retrain ≥ schedule {self.retrain_days}d")
            return True
        return False

    def trigger_retrain(self):
        """
        Run Step 3: ModelTraining end-to-end to refresh models.
        """
        from your_module import ModelTraining  # adjust import
        self.log('INFO', 'Starting automated retraining (Step 3)...')
        trainer = ModelTraining(self.session)
        ok = trainer.execute_step_3()
        if not ok:
            self.errors.append('Retraining failed')
            self.log('ERROR', 'Automated retraining failed; manual intervention needed')
        else:
            self.log('INFO', 'Retraining completed successfully')

    def update_control(self, status: str):
        secs = int((datetime.now() - self.start_time).total_seconds())
        cnt = self.session.sql(
            f"SELECT COUNT(*) as cnt FROM {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL WHERE STEP_NUMBER=5"
        ).collect()[0]['CNT']
        if cnt:
            self.session.sql(f"""
                UPDATE {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL
                SET STATUS='{status}', END_TIME=CURRENT_TIMESTAMP(),
                    EXECUTION_TIME_SECONDS={secs},
                    ERROR_COUNT={len(self.errors)}, WARNING_COUNT={len(self.warnings)}
                WHERE STEP_NUMBER=5
            """).collect()
        else:
            self.session.sql(f"""
                INSERT INTO {FORECAST_CONFIG['forecast_schema']}.PROCESS_CONTROL
                (STEP_NUMBER,STEP_NAME,STATUS,START_TIME,END_TIME,EXECUTION_TIME_SECONDS,ERROR_COUNT,WARNING_COUNT)
                VALUES (5,'MONITOR_RETRAIN','{status}',CURRENT_TIMESTAMP(),CURRENT_TIMESTAMP(),
                {secs},{len(self.errors)},{len(self.warnings)})
            """).collect()

    def execute_step_5(self) -> bool:
        self.log('INFO', '=== STEP 5 START ===')
        self.update_control('RUNNING')
        if not self.validate_step4():
            self.update_control('FAILED')
            return False

        metrics = self.fetch_latest_metrics()
        if not self.performance_ok(metrics) or self.needs_scheduled_retrain():
            self.trigger_retrain()

        final = 'COMPLETED' if not self.errors else 'FAILED'
        self.update_control(final)
        self.log('INFO', f"STEP 5 { 'succeeded' if final=='COMPLETED' else 'failed' }")
        return not self.errors
